{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44beaaf-133c-46f6-8527-ffbc59d75058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e66841-6d80-4a20-8de2-85c169de3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input_data = None \n",
    "        self.output_data = None\n",
    "    def forward(self,X):\n",
    "        pass\n",
    "    def compute_gradient(self,output_error):\n",
    "        pass\n",
    "    def apply_gradient(self,learning_rate,gradients=[]):\n",
    "        pass\n",
    "    def get_gradients(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c0044e7-1656-4818-91df-7c040a01416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def activation(self, X):\n",
    "        raise NotImplementedError(\"Function must be implemented in subclass\")\n",
    "\n",
    "    def d_activation(self, X):\n",
    "        raise NotImplementedError(\"Function must be implemented in subclass\")\n",
    "\n",
    "    def apply_gradient(self,learning_rate,gradients=[]):\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input_data = X\n",
    "        self.output_data = self.activation(X)  # This should correctly update output_data in child class\n",
    "        return self.output_data\n",
    "\n",
    "    def compute_gradient(self, output_error):\n",
    "        input_error = self.d_activation(self.input_data) * output_error\n",
    "        return {'input_error': input_error, 'gradients': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d336832-f933-47c6-a906-a8c1523bd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.W = np.random.uniform(size=(self.input_size,self.output_size)) -0.5\n",
    "        self.b = np.zeros((1,self.output_size))\n",
    "        \n",
    "        self.dW = np.ones(shape=(self.input_size,self.output_size))\n",
    "        self.db = np.ones(shape=(1,self.output_size))\n",
    "\n",
    "    def forward(self,X):\n",
    "        self.input_data = X\n",
    "        self.output_data = np.dot(X,self.W) + self.b\n",
    "        return self.output_data\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return [self.dW,self.db]\n",
    "\n",
    "    def compute_gradient(self,output_error):\n",
    "        self.dW = np.dot(self.input_data.transpose(),output_error)\n",
    "        self.db = np.sum(output_error,axis=0).reshape(1,self.output_data.shape[1])\n",
    "        input_error = np.dot(output_error,self.W.transpose()) \n",
    "        return {\"input_error\" : input_error, \"gradients\" : [self.dW,self.db]}\n",
    "                \n",
    "\n",
    "    def apply_gradient(self,learning_rate,gradients=None):\n",
    "        #Check if the optimizer is sending you custom gradients or use the custom ones\n",
    "        if gradients is None:\n",
    "            self.W = self.W - (learning_rate*self.dW)\n",
    "            self.b = self.b - (learning_rate*self.db)\n",
    "        else:\n",
    "            dW = gradients[0]\n",
    "            db = gradients[1]\n",
    "            self.W = self.W - (learning_rate*dW)\n",
    "            self.b = self.b - (learning_rate*db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5441d62-3e28-417e-b0a1-7c0eb8295548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def activation(self,X):\n",
    "        return X*(X>0)\n",
    "\n",
    "    def d_activation(self,X):\n",
    "        return (X>0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375adf27-392d-499a-8ea6-4c49b6fbc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):        \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def activation(self, X):\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def d_activation(self, X):\n",
    "        s = self.output_data\n",
    "        return s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5617fda2-1b97-477e-a5aa-d9700333dd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def activation(self,X):\n",
    "        return np.tanh(X)\n",
    "        \n",
    "    def d_activation(self,X):\n",
    "        t = self.output_data\n",
    "        return  1. -(t**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ad3fede-9e64-4fb3-8b5a-22bf7f7d80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self,X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b08a099c-002c-4b30-94ee-39d23d97aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def mse(self,y_true,y_pred):\n",
    "        out = y_pred - y_true\n",
    "        out = out**2\n",
    "        return np.mean(out)\n",
    "\n",
    "    def compute_gradient(self,y_true,y_pred):\n",
    "        out = (2/len(y_true))*(y_pred-y_true)\n",
    "        return out\n",
    "\n",
    "    def compute_loss(self,y_true,y_pred):\n",
    "        return self.mse(y_true,y_pred)\n",
    "\n",
    "    def loss(self,y_true,y_pred):\n",
    "        return (self.compute_loss(y_true,y_pred),self.compute_gradient(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e5d46aa-1103-48d5-8e95-3e1534f2b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss:\n",
    "    def __init__(self, epsilon=1e-15):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _clip_probabilities(self, y_pred):\n",
    "        return np.clip(y_pred, self.epsilon, 1 - self.epsilon)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        y_pred = self._clip_probabilities(y_pred)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def compute_gradient(self, y_true, y_pred):\n",
    "        y_pred = self._clip_probabilities(y_pred)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "    def loss(self,y_true,y_pred):\n",
    "        return (self.compute_loss(y_true,y_pred),self.compute_gradient(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f00722-ef8d-400f-a45d-e83a6b78ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleOptimizer:\n",
    "    def __init__(self,layers,lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "    \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            output_error = self.layers[i].compute_gradient(output_error)['input_error']\n",
    "            \n",
    "        for i in range(1,len(self.layers)+1):\n",
    "            self.layers[-i].apply_gradient(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97d25bf1-b3b6-4061-9d55-0e4d5ee92383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOptimizer:\n",
    "    def __init__(self,layers,lr=0.01,beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.beta = beta\n",
    "        self.V_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "    \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[i]\n",
    "            out = layer.compute_gradient(output_error)\n",
    "            output_error = out['input_error']\n",
    "            gradients = out['gradients']\n",
    "            for j in range(len(gradients)):\n",
    "                self.V_gradients[i][j] = self.beta*self.V_gradients[i][j] + (1-self.beta)*gradients[j]\n",
    "                \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[i].apply_gradient(self.lr,self.V_gradients[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65c52d9c-84f8-4b79-956f-63cb2da0ae7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self,layers,lr=0.01,beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.beta = beta\n",
    "        self.epsilon = 1e-8\n",
    "        self.S_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "        \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        final_gradients = {i:[] for i in range(len(layers))}\n",
    "        \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[i]\n",
    "            out = layer.compute_gradient(output_error)\n",
    "            output_error = out['input_error']\n",
    "            gradients = out['gradients']\n",
    "            \n",
    "            for j in range(len(gradients)):\n",
    "                self.S_gradients[i][j] = self.beta*self.S_gradients[i][j] + (1-self.beta)*(gradients[j]**2)\n",
    "                final_gradients[i].append(gradients[j]/(np.sqrt(self.S_gradients[i][j])) + self.epsilon)\n",
    "\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[i].apply_gradient(self.lr,final_gradients[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b430ab84-d71a-4418-a8f8-0ccb14d2f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self,layers,lr=0.01,beta_s=0.99,beta_v=0.9):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        \n",
    "        self.beta_s = beta_s\n",
    "        self.beta_v = beta_v\n",
    "        \n",
    "        self.t = 1\n",
    "        \n",
    "        self.epsilon = 1e-8\n",
    "        \n",
    "        self.S_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "        self.V_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "\n",
    "        \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        final_gradients = {i:[] for i in range(len(layers))}\n",
    "        \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[i]\n",
    "            out = layer.compute_gradient(output_error)\n",
    "            output_error = out['input_error']\n",
    "            gradients = out['gradients']\n",
    "            \n",
    "            for j in range(len(gradients)):\n",
    "                \n",
    "                self.S_gradients[i][j] = (self.beta_s*self.S_gradients[i][j]) + ((1-self.beta_s)*(gradients[j]**2))\n",
    "                corrected_s = self.S_gradients[i][j]/(1-(self.beta_s**self.t))\n",
    "\n",
    "                self.V_gradients[i][j] = (self.beta_v*self.V_gradients[i][j]) + ((1-self.beta_v)*gradients[j])\n",
    "                corrected_v = self.V_gradients[i][j]/(1-(self.beta_v**self.t))\n",
    "\n",
    "                final_gradients[i].append(corrected_v/(np.sqrt(corrected_s) + self.epsilon))\n",
    "            \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[i].apply_gradient(self.lr,final_gradients[i])\n",
    "        self.t += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ddbe0-fed8-4829-b14b-a8f3de65848d",
   "metadata": {},
   "source": [
    "<h3>Solving the Xor problem</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f76337-ec22-41f9-aafd-255011d42172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "y = np.array([0,0,1,1]).reshape(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4d8d9-2ed0-4cf3-b699-0a6147f3237c",
   "metadata": {},
   "source": [
    "<h4>Here we can see how Logistic Regression fails to solve the Xor Problem</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dd9cc91-b666-40ff-bbb9-47c03af1a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y.reshape(4,))\n",
    "lr_pred = lr.predict(X)\n",
    "print(lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec9415-83d7-4004-9ec9-324eca30c6fa",
   "metadata": {},
   "source": [
    "<h4>Now let's check our Neural Network</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad50f2b4-bad4-4566-bf37-d094fed45d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,optimizer,loss,epochs,print_every):\n",
    "    for e in range(epochs):\n",
    "        output = model.forward(X)\n",
    "        error,error_gradient = loss.loss(y,output)\n",
    "        optimizer.step(error_gradient)\n",
    "        if e%print_every == 0:\n",
    "            print(\"epoch: \", e , \"\\t Model error: \", np.round(error,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd3c5b79-77cf-4428-bccc-c083a025a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.01)\n",
    "loss = CELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf1feb4-8516-41f3-901c-d517ddfc2292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.67846\n",
      "epoch:  200 \t Model error:  0.58271\n",
      "epoch:  400 \t Model error:  0.35974\n",
      "epoch:  600 \t Model error:  0.14253\n",
      "epoch:  800 \t Model error:  0.05072\n",
      "epoch:  1000 \t Model error:  0.02397\n",
      "epoch:  1200 \t Model error:  0.01418\n",
      "epoch:  1400 \t Model error:  0.00958\n",
      "epoch:  1600 \t Model error:  0.00704\n",
      "epoch:  1800 \t Model error:  0.00546\n"
     ]
    }
   ],
   "source": [
    "train(NN,optim,loss,2_000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b30e8d1-d730-4f09-a1a3-f25847a808d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :  [0 0 1 1]\n",
      "y is :           [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \" , np.round(NN.forward(X).reshape(4,)).astype(int))\n",
    "print(\"y is :          \" , y.reshape(4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea750e8e-32e1-4258-880a-7c2f424cad59",
   "metadata": {},
   "source": [
    "<h3>Good Job!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c183276-7e1a-4cd4-863f-1039dc07346e",
   "metadata": {},
   "source": [
    "<h3>Comparing SimpleOptimizer to MomentumOptimizer to RMSPropOptimizer vs Adam</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4b75ad-9368-4525-b161-dfa4d45f9779",
   "metadata": {},
   "source": [
    "<h4>Simple optimizer</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6de1929-4255-4a4f-b89e-395c66fc7723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.70924\n",
      "epoch:  200 \t Model error:  0.61952\n",
      "epoch:  400 \t Model error:  0.43136\n",
      "epoch:  600 \t Model error:  0.18784\n",
      "epoch:  800 \t Model error:  0.06411\n",
      "epoch:  1000 \t Model error:  0.02912\n",
      "epoch:  1200 \t Model error:  0.01672\n",
      "epoch:  1400 \t Model error:  0.0111\n",
      "epoch:  1600 \t Model error:  0.00808\n",
      "epoch:  1800 \t Model error:  0.00622\n"
     ]
    }
   ],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.01)\n",
    "loss = CELoss()\n",
    "train(NN,optim,loss,2_000,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adce5e5-8f65-48b6-aa1b-a49d75d8b1b5",
   "metadata": {},
   "source": [
    "<h4>Using Momentum</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47419137-b121-46ca-a7e8-5d3eccc078ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.70478\n",
      "epoch:  200 \t Model error:  0.69316\n",
      "epoch:  400 \t Model error:  0.69315\n",
      "epoch:  600 \t Model error:  0.69315\n",
      "epoch:  800 \t Model error:  0.69315\n",
      "epoch:  1000 \t Model error:  0.69315\n",
      "epoch:  1200 \t Model error:  0.69315\n",
      "epoch:  1400 \t Model error:  0.69315\n",
      "epoch:  1600 \t Model error:  0.69315\n",
      "epoch:  1800 \t Model error:  0.69315\n"
     ]
    }
   ],
   "source": [
    "momentum_layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "momentum_NN = NeuralNetwork(layers=momentum_layers)\n",
    "momentum_optim = MomentumOptimizer(momentum_NN.layers,lr=0.01)\n",
    "momentum_loss = CELoss()\n",
    "train(momentum_NN,momentum_optim,momentum_loss,2_000,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa9145b-151c-43c8-a210-6fac2581c591",
   "metadata": {},
   "source": [
    "<h4>RMSProp</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d23c2eb0-d736-4278-83a0-208811334a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.69107\n",
      "epoch:  200 \t Model error:  0.00035\n",
      "epoch:  400 \t Model error:  0.0\n",
      "epoch:  600 \t Model error:  0.0\n",
      "epoch:  800 \t Model error:  0.0\n",
      "epoch:  1000 \t Model error:  0.0\n",
      "epoch:  1200 \t Model error:  0.0\n",
      "epoch:  1400 \t Model error:  0.0\n",
      "epoch:  1600 \t Model error:  0.0\n",
      "epoch:  1800 \t Model error:  0.0\n"
     ]
    }
   ],
   "source": [
    "rmsprop_layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "rmsprop_NN = NeuralNetwork(layers=rmsprop_layers)\n",
    "rmsprop_optim = RMSProp(rmsprop_NN.layers,lr=0.01)\n",
    "rmsprop_loss = CELoss()\n",
    "train(rmsprop_NN,rmsprop_optim,rmsprop_loss,2_000,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75637df1-5bef-4763-9f79-fe6fa689550a",
   "metadata": {},
   "source": [
    "<h4>Adam</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c437d33-2f61-403b-8333-f45b63207f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.67777\n",
      "epoch:  200 \t Model error:  0.55636\n",
      "epoch:  400 \t Model error:  0.02015\n",
      "epoch:  600 \t Model error:  0.0019\n",
      "epoch:  800 \t Model error:  0.00039\n",
      "epoch:  1000 \t Model error:  0.0001\n",
      "epoch:  1200 \t Model error:  3e-05\n",
      "epoch:  1400 \t Model error:  1e-05\n",
      "epoch:  1600 \t Model error:  0.0\n",
      "epoch:  1800 \t Model error:  0.0\n"
     ]
    }
   ],
   "source": [
    "adam_layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "adam_NN = NeuralNetwork(layers=adam_layers)\n",
    "adam_optim = Adam(adam_NN.layers,lr=0.01)\n",
    "adam_loss = CELoss()\n",
    "train(adam_NN,adam_optim,adam_loss,2_000,200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b763c28-edc9-4ffe-afa1-b812e8d356ad",
   "metadata": {},
   "source": [
    "<h3>We can see how the Models using RMSProp or Adam optimizers converge much faster!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f046-0613-444d-b0ab-b856bb2fd1b2",
   "metadata": {},
   "source": [
    "<h4>Verifying that the model works with multiple outputs (we simply expanded y to 2 classes)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff7b01c0-9c24-4ca3-b79d-afaba9e4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "y = np.array([[1,0],[1,0],[0,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3980f4d-e5e6-4824-b36a-4b4d25dfc2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.24924\n",
      "epoch:  200 \t Model error:  0.0958\n",
      "epoch:  400 \t Model error:  0.0225\n",
      "epoch:  600 \t Model error:  0.0088\n",
      "epoch:  800 \t Model error:  0.00428\n"
     ]
    }
   ],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,2) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.1)\n",
    "loss = MSELoss()\n",
    "train(NN,optim,loss,1_000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63aaf85e-059a-4413-ad8f-7373d57e1bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y is :          \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \\n\" , np.round(NN.forward(X)).astype(int))\n",
    "print(\"y is :          \\n\" , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a617e47e-a1c4-4a79-a8f3-c64bfd55228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \t Model error:  0.69014\n",
      "epoch:  200 \t Model error:  0.0\n",
      "epoch:  400 \t Model error:  0.0\n",
      "epoch:  600 \t Model error:  0.0\n",
      "epoch:  800 \t Model error:  0.0\n"
     ]
    }
   ],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,2) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = RMSProp(NN.layers,lr=0.1)\n",
    "loss = CELoss()\n",
    "train(NN,optim,loss,1_000,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3997d1c1-2775-4a5b-80f5-fefb323e8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y is :          \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \\n\" , np.round(NN.forward(X)).astype(int))\n",
    "print(\"y is :          \\n\" , y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
