{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e44beaaf-133c-46f6-8527-ffbc59d75058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e66841-6d80-4a20-8de2-85c169de3f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.is_parameterized = False\n",
    "    def forward(self,X):\n",
    "        pass\n",
    "    def compute_gradient(self,output_error):\n",
    "        pass\n",
    "    def apply_gradient(self,learning_rate,gradients=[]):\n",
    "        pass\n",
    "    def get_gradients(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d336832-f933-47c6-a906-a8c1523bd849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super().__init__()\n",
    "        self.input_data = None \n",
    "        self.output_data = None\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.is_parameterized = True\n",
    "\n",
    "        \n",
    "        self.W = np.random.uniform(size=(self.input_size,self.output_size))-0.5\n",
    "        self.b = np.random.uniform(size=(1,self.output_size))-0.5\n",
    "        \n",
    "        self.dW = np.ones(shape=(self.input_size,self.output_size))\n",
    "        self.db = np.ones(shape=(1,self.output_size))\n",
    "\n",
    "    def forward(self,X):\n",
    "        self.input_data = X\n",
    "        self.output_data = np.dot(X,self.W) + self.b\n",
    "        return self.output_data\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return [self.dW,self.db]\n",
    "\n",
    "    def compute_gradient(self,output_error):\n",
    "        self.dW = np.dot(self.input_data.transpose(),output_error)\n",
    "        self.db = np.sum(output_error,axis=0).reshape(1,self.output_data.shape[1])\n",
    "        return {\"input_error\" : np.dot(output_error,self.W.transpose()) , \"gradients\" : [self.dW,self.db]}\n",
    "                \n",
    "\n",
    "    def apply_gradient(self,learning_rate,gradients=None):\n",
    "        if gradients is None:\n",
    "            self.W = self.W - (learning_rate*self.dW)\n",
    "            self.b = self.b - (learning_rate*self.db)\n",
    "        else:\n",
    "            dW = gradients[0]\n",
    "            db = gradients[1]\n",
    "            self.W = self.W - (learning_rate*dW)\n",
    "            self.b = self.b - (learning_rate*db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5441d62-3e28-417e-b0a1-7c0eb8295548",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_data = None \n",
    "\n",
    "    def _relu(self,X):\n",
    "        return X*(X>0)\n",
    "\n",
    "    def _d_relu(self,X):\n",
    "        return (X>0).astype(int)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        self.input_data = X\n",
    "        return self._relu(X)\n",
    "\n",
    "    def compute_gradient(self,output_error):\n",
    "        return {'input_error' : self._d_relu(self.input_data) * output_error , 'gradients' : []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375adf27-392d-499a-8ea6-4c49b6fbc6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Layer):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_data = None \n",
    "        self.output_data = None\n",
    "        \n",
    "    def _sigmoid(self,X):\n",
    "        return 1/(1+(np.exp(-X)))\n",
    "\n",
    "    def _d_sigmoid(self,s):\n",
    "        return s*(1-s)\n",
    "            \n",
    "    def forward(self,X):\n",
    "        self.input_data = X\n",
    "        self.output_data = self._sigmoid(X)\n",
    "        return self.output_data\n",
    "        \n",
    "    def compute_gradient(self,output_error):\n",
    "        return {'input_error' : self._d_sigmoid(self.output_data)*output_error , 'gradients' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ad3fede-9e64-4fb3-8b5a-22bf7f7d80bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self,layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self,X):\n",
    "        output = X\n",
    "        for layer in self.layers:\n",
    "            output = layer.forward(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b08a099c-002c-4b30-94ee-39d23d97aa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def mse(self,y_true,y_pred):\n",
    "        out = y_pred - y_true\n",
    "        out = out**2\n",
    "        return np.mean(out)\n",
    "\n",
    "    def compute_gradient(self,y_true,y_pred):\n",
    "        out = (2/len(y_true))*(y_pred-y_true)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "\n",
    "    def compute_loss(self,y_true,y_pred):\n",
    "        return self.mse(y_true,y_pred)\n",
    "\n",
    "    def loss(self,y_true,y_pred):\n",
    "        return (self.compute_loss(y_true,y_pred),self.compute_gradient(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5d46aa-1103-48d5-8e95-3e1534f2b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CELoss:\n",
    "    def __init__(self, epsilon=1e-15):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _clip_probabilities(self, y_pred):\n",
    "        return np.clip(y_pred, self.epsilon, 1 - self.epsilon)\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        y_pred = self._clip_probabilities(y_pred)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "    def compute_gradient(self, y_true, y_pred):\n",
    "        y_pred = self._clip_probabilities(y_pred)\n",
    "        return (y_pred - y_true) / (y_pred * (1 - y_pred))\n",
    "\n",
    "    def loss(self,y_true,y_pred):\n",
    "        return (self.compute_loss(y_true,y_pred),self.compute_gradient(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6f00722-ef8d-400f-a45d-e83a6b78ce51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleOptimizer:\n",
    "    def __init__(self,layers,lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "    \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            output_error = self.layers[i].compute_gradient(output_error)['input_error']\n",
    "            \n",
    "        for i in range(1,len(self.layers)+1):\n",
    "            self.layers[-i].apply_gradient(self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97d25bf1-b3b6-4061-9d55-0e4d5ee92383",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumOptimizer:\n",
    "    def __init__(self,layers,lr=0.01,beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.beta = beta\n",
    "        self.V_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "    \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[i]\n",
    "            out = layer.compute_gradient(output_error)\n",
    "            output_error = out['input_error']\n",
    "            gradients = out['gradients']\n",
    "            for j in range(len(gradients)):\n",
    "                self.V_gradients[i][j] = self.beta*self.V_gradients[i][j] + (1-self.beta)*gradients[j]\n",
    "                \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[i].apply_gradient(self.lr,self.V_gradients[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b430ab84-d71a-4418-a8f8-0ccb14d2f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSProp:\n",
    "    def __init__(self,layers,lr=0.01,beta=0.9):\n",
    "        self.lr = lr\n",
    "        self.layers = layers\n",
    "        self.beta = beta\n",
    "        self.epsilon = 1e-8\n",
    "        self.S_gradients = {i:layers[i].get_gradients() for i in range(len(layers))}\n",
    "        \n",
    "    def step(self,error):\n",
    "        output_error = error\n",
    "        final_gradients = {i:[] for i in range(len(layers))}\n",
    "        \n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            layer = self.layers[i]\n",
    "            out = layer.compute_gradient(output_error)\n",
    "            output_error = out['input_error']\n",
    "            gradients = out['gradients']\n",
    "            \n",
    "            for j in range(len(gradients)):\n",
    "                self.S_gradients[i][j] = self.beta*self.S_gradients[i][j] + (1-self.beta)*(gradients[j]**2)\n",
    "                final_gradients[i].append(gradients[j]/(np.sqrt(self.S_gradients[i][j])) + self.epsilon)\n",
    "\n",
    "        for i in range(len(self.layers)-1,-1,-1):\n",
    "            self.layers[i].apply_gradient(self.lr,final_gradients[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47ddbe0-fed8-4829-b14b-a8f3de65848d",
   "metadata": {},
   "source": [
    "<h3>Solving the Xor problem</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8f76337-ec22-41f9-aafd-255011d42172",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "y = np.array([0,0,1,1]).reshape(4,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4d8d9-2ed0-4cf3-b699-0a6147f3237c",
   "metadata": {},
   "source": [
    "<h4>Here we can see how Logistic Regression fails to solve the Xor Problem</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd9cc91-b666-40ff-bbb9-47c03af1a496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X,y.reshape(4,))\n",
    "lr_pred = lr.predict(X)\n",
    "print(lr_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aec9415-83d7-4004-9ec9-324eca30c6fa",
   "metadata": {},
   "source": [
    "<h4>Now let's check our Neural Network</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd3c5b79-77cf-4428-bccc-c083a025a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.01)\n",
    "loss = CELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccf1feb4-8516-41f3-901c-d517ddfc2292",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \n",
      " \t train error:  0.68313\n",
      "epoch:  100 \n",
      " \t train error:  0.62579\n",
      "epoch:  200 \n",
      " \t train error:  0.53624\n",
      "epoch:  300 \n",
      " \t train error:  0.39467\n",
      "epoch:  400 \n",
      " \t train error:  0.22658\n",
      "epoch:  500 \n",
      " \t train error:  0.11362\n",
      "epoch:  600 \n",
      " \t train error:  0.06189\n",
      "epoch:  700 \n",
      " \t train error:  0.03794\n",
      "epoch:  800 \n",
      " \t train error:  0.02596\n",
      "epoch:  900 \n",
      " \t train error:  0.01886\n"
     ]
    }
   ],
   "source": [
    "for e in range(1_000):\n",
    "    output = NN.forward(X)\n",
    "    error,error_gradient = loss.loss(y,output)\n",
    "    optim.step(error_gradient)\n",
    "    if e%100 == 0:\n",
    "        print(\"epoch: \", e , \"\\n \\t train error: \", np.round(error,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b30e8d1-d730-4f09-a1a3-f25847a808d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is :  [0 0 1 1]\n",
      "y is :           [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \" , np.round(NN.forward(X).reshape(4,)).astype(int))\n",
    "print(\"y is :          \" , y.reshape(4,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea750e8e-32e1-4258-880a-7c2f424cad59",
   "metadata": {},
   "source": [
    "<h3>Good Job!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c183276-7e1a-4cd4-863f-1039dc07346e",
   "metadata": {},
   "source": [
    "<h3>Comparing regular optimizer to Momentum Optimizer to RMSProp Optimizer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64c9392f-53f4-4419-927f-beedbe2dcc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.01)\n",
    "loss = CELoss()\n",
    "\n",
    "momentum_layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "momentum_NN = NeuralNetwork(layers=momentum_layers)\n",
    "momentum_optim = MomentumOptimizer(momentum_NN.layers,lr=0.01)\n",
    "momentum_loss = CELoss()\n",
    "\n",
    "rmsprop_layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,1) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "rmsprop_NN = NeuralNetwork(layers=rmsprop_layers)\n",
    "rmsprop_optim = RMSProp(rmsprop_NN.layers,lr=0.01)\n",
    "rmsprop_loss = CELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "065ce6ef-fb85-4e14-9663-6fbc4d4e836a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \n",
      " \t Model with regular optimizer train error:  0.71302 \n",
      " \t Model with momentum optimizer train error:  0.69004 \n",
      " \t Model with rmsprop optimizer train error:  0.69658\n",
      "epoch:  50 \n",
      " \t Model with regular optimizer train error:  0.69395 \n",
      " \t Model with momentum optimizer train error:  0.65487 \n",
      " \t Model with rmsprop optimizer train error:  0.61106\n",
      "epoch:  100 \n",
      " \t Model with regular optimizer train error:  0.68529 \n",
      " \t Model with momentum optimizer train error:  0.62673 \n",
      " \t Model with rmsprop optimizer train error:  0.16216\n",
      "epoch:  150 \n",
      " \t Model with regular optimizer train error:  0.67623 \n",
      " \t Model with momentum optimizer train error:  0.59469 \n",
      " \t Model with rmsprop optimizer train error:  0.01128\n",
      "epoch:  200 \n",
      " \t Model with regular optimizer train error:  0.66257 \n",
      " \t Model with momentum optimizer train error:  0.55294 \n",
      " \t Model with rmsprop optimizer train error:  0.00067\n",
      "epoch:  250 \n",
      " \t Model with regular optimizer train error:  0.64865 \n",
      " \t Model with momentum optimizer train error:  0.47595 \n",
      " \t Model with rmsprop optimizer train error:  4e-05\n",
      "epoch:  300 \n",
      " \t Model with regular optimizer train error:  0.62823 \n",
      " \t Model with momentum optimizer train error:  0.38001 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  350 \n",
      " \t Model with regular optimizer train error:  0.59941 \n",
      " \t Model with momentum optimizer train error:  0.27312 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  400 \n",
      " \t Model with regular optimizer train error:  0.55599 \n",
      " \t Model with momentum optimizer train error:  0.17886 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  450 \n",
      " \t Model with regular optimizer train error:  0.49172 \n",
      " \t Model with momentum optimizer train error:  0.11485 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  500 \n",
      " \t Model with regular optimizer train error:  0.40877 \n",
      " \t Model with momentum optimizer train error:  0.07817 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  550 \n",
      " \t Model with regular optimizer train error:  0.31516 \n",
      " \t Model with momentum optimizer train error:  0.05606 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  600 \n",
      " \t Model with regular optimizer train error:  0.2242 \n",
      " \t Model with momentum optimizer train error:  0.04206 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  650 \n",
      " \t Model with regular optimizer train error:  0.15418 \n",
      " \t Model with momentum optimizer train error:  0.03284 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  700 \n",
      " \t Model with regular optimizer train error:  0.10516 \n",
      " \t Model with momentum optimizer train error:  0.02644 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  750 \n",
      " \t Model with regular optimizer train error:  0.07473 \n",
      " \t Model with momentum optimizer train error:  0.02184 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  800 \n",
      " \t Model with regular optimizer train error:  0.05505 \n",
      " \t Model with momentum optimizer train error:  0.01839 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  850 \n",
      " \t Model with regular optimizer train error:  0.04221 \n",
      " \t Model with momentum optimizer train error:  0.01576 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  900 \n",
      " \t Model with regular optimizer train error:  0.03343 \n",
      " \t Model with momentum optimizer train error:  0.01369 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n",
      "epoch:  950 \n",
      " \t Model with regular optimizer train error:  0.02725 \n",
      " \t Model with momentum optimizer train error:  0.01205 \n",
      " \t Model with rmsprop optimizer train error:  0.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(1_000):\n",
    "    output = NN.forward(X)\n",
    "    error,error_gradient = loss.loss(y,output)\n",
    "    optim.step(error_gradient)\n",
    "\n",
    "    momentum_output = momentum_NN.forward(X)\n",
    "    momentum_error,momentum_error_gradient = loss.loss(y,momentum_output)\n",
    "    momentum_optim.step(momentum_error_gradient)\n",
    "    \n",
    "    rmsprop_output = rmsprop_NN.forward(X)\n",
    "    rmsprop_error,rmsprop_error_gradient = loss.loss(y,rmsprop_output)\n",
    "    rmsprop_optim.step(rmsprop_error_gradient)\n",
    "    \n",
    "    if e%50 == 0:\n",
    "        print(\"epoch: \", e , \"\\n \\t Model with regular optimizer train error: \", np.round(error,5), \n",
    "              \"\\n \\t Model with momentum optimizer train error: \", np.round(momentum_error,5),\n",
    "        \"\\n \\t Model with rmsprop optimizer train error: \", np.round(rmsprop_error,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b763c28-edc9-4ffe-afa1-b812e8d356ad",
   "metadata": {},
   "source": [
    "<h3>We can see how the Model using the RMSProp optimizer converges faster!</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f046-0613-444d-b0ab-b856bb2fd1b2",
   "metadata": {},
   "source": [
    "<h4>Verifying that the model works with multiple outputs (we simply expanded y to 2 classes)</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff7b01c0-9c24-4ca3-b79d-afaba9e4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0,0],[1,1],[0,1],[1,0]])\n",
    "y = np.array([[1,0],[1,0],[0,1],[0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3980f4d-e5e6-4824-b36a-4b4d25dfc2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,2) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = SimpleOptimizer(NN.layers,lr=0.1)\n",
    "loss = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f875aac-187d-454d-b08d-c1bb08ddfd51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \n",
      " \t train error:  0.28318\n",
      "epoch:  100 \n",
      " \t train error:  0.22913\n",
      "epoch:  200 \n",
      " \t train error:  0.14376\n",
      "epoch:  300 \n",
      " \t train error:  0.03114\n",
      "epoch:  400 \n",
      " \t train error:  0.00906\n",
      "epoch:  500 \n",
      " \t train error:  0.00433\n",
      "epoch:  600 \n",
      " \t train error:  0.00264\n",
      "epoch:  700 \n",
      " \t train error:  0.00183\n",
      "epoch:  800 \n",
      " \t train error:  0.00137\n",
      "epoch:  900 \n",
      " \t train error:  0.00108\n"
     ]
    }
   ],
   "source": [
    "for e in range(1_000):\n",
    "    output = NN.forward(X)\n",
    "    error,error_gradient = loss.loss(y,output)\n",
    "    optim.step(error_gradient)\n",
    "    if e%100 == 0:\n",
    "        print(\"epoch: \", e , \"\\n \\t train error: \", np.round(error,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63aaf85e-059a-4413-ad8f-7373d57e1bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y is :          \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \\n\" , np.round(NN.forward(X)).astype(int))\n",
    "print(\"y is :          \\n\" , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a617e47e-a1c4-4a79-a8f3-c64bfd55228f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [Linear(2,16), \n",
    "          ReLU(),\n",
    "          Linear(16,8) ,\n",
    "          ReLU(),\n",
    "          Linear(8,2) ,\n",
    "          Sigmoid()]\n",
    "\n",
    "NN = NeuralNetwork(layers=layers)\n",
    "optim = RMSProp(NN.layers,lr=0.1)\n",
    "loss = CELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e7eadca-44d2-454a-b402-4665ad40d3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0 \n",
      " \t train error:  0.71649\n",
      "epoch:  100 \n",
      " \t train error:  9e-05\n",
      "epoch:  200 \n",
      " \t train error:  0.0\n",
      "epoch:  300 \n",
      " \t train error:  0.0\n",
      "epoch:  400 \n",
      " \t train error:  0.0\n",
      "epoch:  500 \n",
      " \t train error:  0.0\n",
      "epoch:  600 \n",
      " \t train error:  0.0\n",
      "epoch:  700 \n",
      " \t train error:  0.0\n",
      "epoch:  800 \n",
      " \t train error:  0.0\n",
      "epoch:  900 \n",
      " \t train error:  0.0\n"
     ]
    }
   ],
   "source": [
    "for e in range(1_000):\n",
    "    output = NN.forward(X)\n",
    "    error,error_gradient = loss.loss(y,output)\n",
    "    optim.step(error_gradient)\n",
    "    if e%100 == 0:\n",
    "        print(\"epoch: \", e , \"\\n \\t train error: \", np.round(error,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3997d1c1-2775-4a5b-80f5-fefb323e8158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction is : \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "y is :          \n",
      " [[1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Prediction is : \\n\" , np.round(NN.forward(X)).astype(int))\n",
    "print(\"y is :          \\n\" , y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
